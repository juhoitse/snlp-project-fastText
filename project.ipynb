{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "360ccb85-1646-44d7-96f6-c556d1cb1022",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Download tokenizer\n",
    "#nltk.download('punkt_tab')\n",
    "\n",
    "# Load dataset (assumes a CSV file with 'text' and 'label' columns)\n",
    "df = pd.read_csv(\"train.tsv\", sep='\\t')  # Replace with actual file\n",
    "#print(df.head())\n",
    "# Tokenization\n",
    "tokenizer = get_tokenizer(\"basic_english\")  # Or use nltk.word_tokenize\n",
    "\n",
    "# Build Vocabulary\n",
    "def yield_tokens(data):\n",
    "    for text in data:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab_whole = build_vocab_from_iterator(yield_tokens(df['text']), specials=[\"<pad>\", \"<unk>\"])\n",
    "vocab_whole.set_default_index(vocab_whole[\"<unk>\"])  # Handle out-of-vocabulary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14f16b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_ngrams(text, n=3):\n",
    "    \"\"\"\n",
    "    Tokenizes text into words using NLTK, then generates character-level n-grams (subwords).\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    \n",
    "    # Tokenize text into words using NLTK\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    for word in words:\n",
    "        # Add special boundary markers (like FastText)\n",
    "        word = f\"<s>{word}</s>\"\n",
    "\n",
    "        # Generate n-grams for each word\n",
    "        ngrams = [word[i:i+n] for i in range(len(word) - n + 1)]\n",
    "\n",
    "        tokens.extend(ngrams)  # Add n-grams to the list\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Function to generate n-gram tokens from a dataset\n",
    "def yield_ngram_tokens(data, n=3):\n",
    "    \"\"\"\n",
    "    Generator function to yield n-gram tokens for each text in the dataset.\n",
    "    \"\"\"\n",
    "    for text in data:\n",
    "        yield tokenize_with_ngrams(text, n)\n",
    "\n",
    "# Build vocabulary from n-grams\n",
    "vocab_sub = build_vocab_from_iterator(yield_ngram_tokens(df['text']), specials=[\"<pad>\", \"<unk>\"])\n",
    "\n",
    "# Set default index for unknown tokens\n",
    "vocab_sub.set_default_index(vocab_sub[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0675e1c-51a4-4190-8b88-7f8c5d461b6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ToxicCommentDataset_sub(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len=100):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize and convert to numerical indices\n",
    "        tokenized = tokenize_with_ngrams(text, 3)\n",
    "        indexed = [self.vocab[token] for token in tokenized]\n",
    "\n",
    "        # Pad/truncate sequence to max_len\n",
    "        if len(indexed) < self.max_len:\n",
    "            indexed += [self.vocab[\"<pad>\"]] * (self.max_len - len(indexed))\n",
    "        else:\n",
    "            indexed = indexed[:self.max_len]\n",
    "\n",
    "        return torch.tensor(indexed, dtype=torch.long), torch.tensor(label, dtype=torch.float32)\n",
    "    \n",
    "class ToxicCommentDataset_whole(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len=100):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize and convert to numerical indices\n",
    "        tokenized = tokenizer(text)\n",
    "        indexed = [self.vocab[token] for token in tokenized]\n",
    "\n",
    "        # Pad/truncate sequence to max_len\n",
    "        if len(indexed) < self.max_len:\n",
    "            indexed += [self.vocab[\"<pad>\"]] * (self.max_len - len(indexed))\n",
    "        else:\n",
    "            indexed = indexed[:self.max_len]\n",
    "\n",
    "        return torch.tensor(indexed, dtype=torch.long), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "# Convert labels to numerical format\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# Create dataset\n",
    "\n",
    "dataset_sub   = ToxicCommentDataset_sub(df['text'].tolist(), df['label'].tolist(), vocab_sub)\n",
    "dataset_whole = ToxicCommentDataset_whole(df['text'].tolist(), df['label'].tolist(), vocab_whole)\n",
    "\n",
    "# DataLoader\n",
    "train_loader_sub   = DataLoader(dataset_sub, batch_size=32, shuffle=True)\n",
    "train_loader_whole = DataLoader(dataset_whole, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "836a8fa1-c282-4a45-aad0-738f795dd1a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FastTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
    "        super(FastTextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n",
    "        avg_embeds = embeds.mean(dim=1)  # Mean pooling over sequence\n",
    "        out = self.fc(avg_embeds)  # Fully connected layer\n",
    "        return torch.sigmoid(out).squeeze(1)  # Binary classification\n",
    "\n",
    "# Initialize model\n",
    "vocab_size_sub = len(vocab_sub)\n",
    "vocab_size_whole = len(vocab_whole)\n",
    "embed_dim = 100  # Can be 50, 100, or 300\n",
    "model_sub = FastTextClassifier(vocab_size_sub, embed_dim, num_classes=1)\n",
    "model_whole = FastTextClassifier(vocab_size_whole, embed_dim, num_classes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15082baa-1def-48aa-a9a3-f17eef0701b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load(\"./trained_model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6dbc41a5-b0cc-4fe9-94ea-e5cde46b8bea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.5450\n",
      "Took 32.42 seconds\n",
      "Epoch [2/10], Loss: 0.3563\n",
      "Took 35.35 seconds\n",
      "Epoch [3/10], Loss: 0.2654\n",
      "Took 35.53 seconds\n",
      "Epoch [4/10], Loss: 0.2216\n",
      "Took 35.46 seconds\n",
      "Epoch [5/10], Loss: 0.1962\n",
      "Took 35.6 seconds\n",
      "Epoch [6/10], Loss: 0.1783\n",
      "Took 35.31 seconds\n",
      "Epoch [7/10], Loss: 0.1654\n",
      "Took 31.45 seconds\n",
      "Epoch [8/10], Loss: 0.1560\n",
      "Took 32.46 seconds\n",
      "Epoch [9/10], Loss: 0.1476\n",
      "Took 37.09 seconds\n",
      "Epoch [10/10], Loss: 0.1395\n",
      "Took 37.1 seconds\n",
      "Epoch [1/10], Loss: 0.5694\n",
      "Took 125.05 seconds\n",
      "Epoch [2/10], Loss: 0.4848\n",
      "Took 126.11 seconds\n",
      "Epoch [3/10], Loss: 0.4630\n",
      "Took 105.56 seconds\n",
      "Epoch [4/10], Loss: 0.4510\n",
      "Took 94.82 seconds\n",
      "Epoch [5/10], Loss: 0.4427\n",
      "Took 93.15 seconds\n",
      "Epoch [6/10], Loss: 0.4364\n",
      "Took 89.96 seconds\n",
      "Epoch [7/10], Loss: 0.4310\n",
      "Took 93.84 seconds\n",
      "Epoch [8/10], Loss: 0.4266\n",
      "Took 92.36 seconds\n",
      "Epoch [9/10], Loss: 0.4230\n",
      "Took 93.35 seconds\n",
      "Epoch [10/10], Loss: 0.4194\n",
      "Took 94.16 seconds\n"
     ]
    }
   ],
   "source": [
    "criterion_sub = nn.BCELoss()\n",
    "optimizer_sub = optim.Adam(model_sub.parameters(), lr=0.001)\n",
    "\n",
    "criterion_whole = nn.BCELoss()\n",
    "optimizer_whole = optim.Adam(model_whole.parameters(), lr=0.001)\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=5, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        start = time.time()\n",
    "        for texts, labels in train_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "        print(\"Took \" + str(round(time.time()-start, 2)) + \" seconds\")\n",
    "\n",
    "# Train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_model(model_whole, train_loader_whole, criterion_whole, optimizer_whole, epochs=10, device=device)\n",
    "torch.save(model_whole.state_dict(), \"./trained_model_whole.pth\")\n",
    "\n",
    "train_model(model_sub, train_loader_sub, criterion_sub, optimizer_sub, epochs=10, device=device)\n",
    "torch.save(model_sub.state_dict(), \"./trained_model_sub.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "14c702c8-5d47-4391-9fe6-b9cc60970d0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Toxic\n"
     ]
    }
   ],
   "source": [
    "def predict(model, text, vocab, tokenizer, max_len=100, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize and convert to indices\n",
    "    tokenized = tokenizer(text)\n",
    "    indexed = [vocab[token] for token in tokenized]\n",
    "\n",
    "    # Pad/truncate sequence\n",
    "    if len(indexed) < max_len:\n",
    "        indexed += [vocab[\"<pad>\"]] * (max_len - len(indexed))\n",
    "    else:\n",
    "        indexed = indexed[:max_len]\n",
    "\n",
    "    # Convert to tensor\n",
    "    input_tensor = torch.tensor(indexed, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    # Get model prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor).item()\n",
    "\n",
    "    return \"Toxic\" if output >= 0.5 else \"Not Toxic\"\n",
    "\n",
    "# Example Prediction\n",
    "sample_text = \"<s>shit</s>\"\n",
    "print(predict(model, sample_text, vocab, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a80ed0-b60b-46be-904d-f362e65fe69a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load test dataset (assuming a CSV file with 'text' and 'label' columns)\n",
    "df_test = pd.read_csv(\"dev.tsv\", sep='\\t')  # Replace with actual test file\n",
    "# Convert labels to numerical format\n",
    "df_test['label'] = df_test['label'].astype(int)\n",
    "\n",
    "df_eng = df_test[df_test['id'].str.contains('eng')]\n",
    "df_ger = df_test[df_test['id'].str.contains('ger')]\n",
    "df_fin = df_test[df_test['id'].str.contains('fin')]\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset_whole     = ToxicCommentDataset_whole(df_test['text'].tolist(), df_test['label'].tolist(), vocab_whole)\n",
    "test_dataset_eng_whole = ToxicCommentDataset_whole(df_eng['text'].tolist(), df_eng['label'].tolist(), vocab_whole)\n",
    "test_dataset_fin_whole = ToxicCommentDataset_whole(df_fin['text'].tolist(), df_fin['label'].tolist(), vocab_whole)\n",
    "test_dataset_ger_whole = ToxicCommentDataset_whole(df_ger['text'].tolist(), df_ger['label'].tolist(), vocab_whole)\n",
    "\n",
    "test_dataset_sub     = ToxicCommentDataset_sub(df_test['text'].tolist(), df_test['label'].tolist(), vocab_sub)\n",
    "test_dataset_eng_sub = ToxicCommentDataset_sub(df_eng['text'].tolist(), df_eng['label'].tolist(), vocab_sub)\n",
    "test_dataset_fin_sub = ToxicCommentDataset_sub(df_fin['text'].tolist(), df_fin['label'].tolist(), vocab_sub)\n",
    "test_dataset_ger_sub = ToxicCommentDataset_sub(df_ger['text'].tolist(), df_ger['label'].tolist(), vocab_sub)\n",
    "\n",
    "# Test DataLoader\n",
    "test_loader_whole     = DataLoader(test_dataset_whole, batch_size=32, shuffle=False)\n",
    "test_loader_eng_whole = DataLoader(test_dataset_eng_whole, batch_size=32, shuffle=False)\n",
    "test_loader_ger_whole = DataLoader(test_dataset_ger_whole, batch_size=32, shuffle=False)\n",
    "test_loader_fin_whole = DataLoader(test_dataset_fin_whole, batch_size=32, shuffle=False)\n",
    "\n",
    "test_loader_sub     = DataLoader(test_dataset_sub, batch_size=32, shuffle=False)\n",
    "test_loader_eng_sub = DataLoader(test_dataset_eng_sub, batch_size=32, shuffle=False)\n",
    "test_loader_ger_sub = DataLoader(test_dataset_ger_sub, batch_size=32, shuffle=False)\n",
    "test_loader_fin_sub = DataLoader(test_dataset_fin_sub, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bbed1ad7-1ffe-41f5-8753-8a5b30c1bdcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full score whole words:\n",
      "Test Accuracy: 0.8501\n",
      "Precision: 0.7620\n",
      "Recall: 0.8183\n",
      "F1 Score: 0.7892\n",
      "\n",
      "English scores whole words:\n",
      "Test Accuracy: 0.9166\n",
      "Precision: 0.9114\n",
      "Recall: 0.8454\n",
      "F1 Score: 0.8772\n",
      "\n",
      "German scores whole words:\n",
      "Test Accuracy: 0.5160\n",
      "Precision: 0.2947\n",
      "Recall: 0.6720\n",
      "F1 Score: 0.4098\n",
      "\n",
      "Finnish scores whole words:\n",
      "Test Accuracy: 0.5400\n",
      "Precision: 0.7339\n",
      "Recall: 0.6067\n",
      "F1 Score: 0.6642\n",
      "\n",
      "Full scores subwords:\n",
      "Test Accuracy: 0.7701\n",
      "Precision: 0.7193\n",
      "Recall: 0.5404\n",
      "F1 Score: 0.6171\n",
      "\n",
      "English scores subwords:\n",
      "Test Accuracy: 0.7842\n",
      "Precision: 0.7375\n",
      "Recall: 0.6020\n",
      "F1 Score: 0.6629\n",
      "\n",
      "German scores subwords:\n",
      "Test Accuracy: 0.7255\n",
      "Precision: 0.3631\n",
      "Recall: 0.1300\n",
      "F1 Score: 0.1915\n",
      "\n",
      "Finnish scores subwords:\n",
      "Test Accuracy: 0.4400\n",
      "Precision: 0.8276\n",
      "Recall: 0.3200\n",
      "F1 Score: 0.4615\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_model(model, test_loader, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in test_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(texts)  # Forward pass\n",
    "            preds = (outputs >= 0.5).long()  # Convert probabilities to binary labels\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Compute Metrics\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Run the evaluation\n",
    "print('Full score whole words:')\n",
    "evaluate_model(model_whole, test_loader_whole)\n",
    "print('\\nEnglish scores whole words:')\n",
    "evaluate_model(model_whole, test_loader_eng_whole)\n",
    "print('\\nGerman scores whole words:')\n",
    "evaluate_model(model_whole, test_loader_ger_whole)\n",
    "print('\\nFinnish scores whole words:')\n",
    "evaluate_model(model_whole, test_loader_fin_whole)\n",
    "\n",
    "print('\\nFull scores subwords:')\n",
    "evaluate_model(model_sub, test_loader_sub)\n",
    "print('\\nEnglish scores subwords:')\n",
    "evaluate_model(model_sub, test_loader_eng_sub)\n",
    "print('\\nGerman scores subwords:')\n",
    "evaluate_model(model_sub, test_loader_ger_sub)\n",
    "print('\\nFinnish scores subwords:')\n",
    "evaluate_model(model_sub, test_loader_fin_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f6f877-d997-4148-9422-90e7512e0363",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLDL2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
